{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 워드 임베딩"
      ],
      "metadata": {
        "id": "hh4oXFYawL0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 벡터화"
      ],
      "metadata": {
        "id": "2mlqPqiPwPIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of words/ DTM(Document-Term Matrix)\n",
        "- Bag of words : 단어의 순서를 고려하지 않고, 단어의 등장 빈도(frequency)만 고려하여 단어를 벡터화 하는 방법\n",
        "- DTM : 문서 간 유사도를 비교하기 위한 행렬, 문서=행/단어=열\n",
        "- 희소 벡터 : 대부분의 값이 0인 벡터\n",
        "- 단어장 : 중복 카운트를 배제한 단어들의 집합\n"
      ],
      "metadata": {
        "id": "CwumaFbU3Sss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF\n",
        "- 단어의 중요도를 계산하기 위한 가중치 기법\n",
        "- 모든 문서에 자주 등장하는 중요하지 않은 단어(예: the, is 등)의 영향은 줄이고, 특정 문서에만 자주 등장하는 단어의 중요도를 강조\n",
        "- TF(단어 빈도): 해당 문서에서 단어가 얼마나 자주 등장했는지\n",
        "- IDF(역문서 빈도): 그 단어가 전체 문서 중 얼마나 희귀한지\n"
      ],
      "metadata": {
        "id": "A8A4gW1C360h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 원-핫 인코딩 구현\n",
        "- 데이터 전처리 : 텍스트 데이터에서 단어들의 집합인 단어장 생성.\n",
        "- 단어장의 모든 단어에 대하여 1~V까지 고유한 정수 부여(인덱스 역할)\n",
        "- 원-핫 벡터 : 원-핫 인코딩을 통해 얻은 벡터\n",
        "- TF = 문장을 구성하는 단어들의 원-핫 벡터들을 모두 더해서 문장의 개수로 나눈것"
      ],
      "metadata": {
        "id": "DUL6eD7t902i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step1. 패키지 설치하기"
      ],
      "metadata": {
        "id": "hn72t-t5-bQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNO1jxdFxnHw",
        "outputId": "463b57a2-f772-412d-b1a1-d67c4ea389cd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.2 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9s2U1-zym29",
        "outputId": "41add52d-fd43-4c8a-fe96-f3e5a22e74bb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "m-kxekL-ykOO",
        "outputId": "8c65341b-f29f-446e-975d-b527a67f278d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "52e3a59da1ff4aa5af733ca4e72f9d4f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Gh41qONxfw1",
        "outputId": "65253594-3b24-4d58-80c4-a922578dc46c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임포트 완료\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "print(\"임포트 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"임금님 귀는 당나귀 귀! 임금님 귀는 당나귀 귀! 실컷~ 소리치고 나니 속이 확 뚫려 살 것 같았어.\"\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oSuk1d9Cxohn",
        "outputId": "d3f77df6-69be-4665-b23a-8f517696d693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'임금님 귀는 당나귀 귀! 임금님 귀는 당나귀 귀! 실컷~ 소리치고 나니 속이 확 뚫려 살 것 같았어.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step2. 전처리"
      ],
      "metadata": {
        "id": "blR_842E-osf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 특수 문자(!, ~, .) 제거\n",
        "- 한국어 정규 표현식  \n",
        "  - regex : [^ㄱ-ㅎㅏ-ㅣ가-힣 ]"
      ],
      "metadata": {
        "id": "SqyNmLN2-3dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg = re.compile(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\")\n",
        "text = reg.sub('', text)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG-H_0dbxtCK",
        "outputId": "2a38bc77-d960-4f61-ebf4-ad41c237c9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임금님 귀는 당나귀 귀 임금님 귀는 당나귀 귀 실컷 소리치고 나니 속이 확 뚫려 살 것 같았어\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step3. 토큰화"
      ],
      "metadata": {
        "id": "fJdk-rHFW2QI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 단어장 구성을 위함.\n",
        "- 한국어는 형태소 분석기를 통해 토큰 단위로 나눔"
      ],
      "metadata": {
        "id": "LUOqcxdZW7XK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "okt=Okt()\n",
        "tokens = okt.morphs(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhtYLtRqxuUZ",
        "outputId": "2673c181-1016-4d2e-a4b3-3660d7609490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['임금님', '귀', '는', '당나귀', '귀', '임금님', '귀', '는', '당나귀', '귀', '실컷', '소리', '치고', '나니', '속이', '확', '뚫려', '살', '것', '같았어']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step4. 단어장 만들기\n",
        "- 빈도수가 높은 단어일수록 낮은 점수를 부여함."
      ],
      "metadata": {
        "id": "CLObZmAHZNKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Counter(tokens) #단어의 빈도 체크\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE4h2XJ7xwZ_",
        "outputId": "ee795248-7850-478d-99a2-68be5f9fb0d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'귀': 4, '임금님': 2, '는': 2, '당나귀': 2, '실컷': 1, '소리': 1, '치고': 1, '나니': 1, '속이': 1, '확': 1, '뚫려': 1, '살': 1, '것': 1, '같았어': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab['임금님']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyTXpAF6xy0z",
        "outputId": "d07b18ae-87d9-4b03-e4d3-42fd9657caf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asQpU9Wgxz1D",
        "outputId": "ec2f3043-2e16-48cf-c306-6b07329d690f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('귀', 4), ('임금님', 2), ('는', 2), ('당나귀', 2), ('실컷', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab에서 각 단어의 첫 번째 요소 word[0] (예: 'apple')를 키로,\n",
        "# 해당 인덱스에 +1 한 값을 값으로 갖는 딕셔너리를 만드는 것.\n",
        "word2idx={word[0] : index+1 for index, word in enumerate(vocab)}\n",
        "print(word2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bg0pBmxx1Up",
        "outputId": "7921265d-d8dd-4549-f041-45fc3d592424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'귀': 1, '임금님': 2, '는': 3, '당나귀': 4, '실컷': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step5.원-핫 벡터 만들기"
      ],
      "metadata": {
        "id": "A5VSAGkXaTEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 특정 단어와 단어장을 입력하면 해당 단어의 원-핫 벡터를 리턴하는 함수\n",
        "def one_hot_encoding(word, word2index):\n",
        "       one_hot_vector = [0]*(len(word2index))\n",
        "       index = word2index[word]\n",
        "       one_hot_vector[index-1] = 1\n",
        "       return one_hot_vector\n",
        "print(\"슝=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW56hIcsyNB6",
        "outputId": "306f0b8b-6aef-4b89-dcf2-5b385dee5913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoding(\"임금님\", word2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HguLlZ58yRlO",
        "outputId": "f1bedc59-71b5-46fa-90cf-87d9ab1cc475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 케라스를 통한 원-핫인코딩"
      ],
      "metadata": {
        "id": "aDJPOIvubCPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "print(\"임포트 완료\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCxcEa94ySQA",
        "outputId": "7dd68963-1f64-49ed-d683-5da6a682c83b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임포트 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [['강아지', '고양이', '강아지'],['애교', '고양이'], ['컴퓨터', '노트북']]\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVYFYnbXyTRb",
        "outputId": "6ac20531-39d8-47dd-f96e-855ce2425570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['강아지', '고양이', '강아지'], ['애교', '고양이'], ['컴퓨터', '노트북']]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()\n",
        "t.fit_on_texts(text)\n",
        "print(t.word_index) # 각 단어에 대한 인코딩 결과 출력."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsmAvD5wyVv1",
        "outputId": "83d7fc12-a6b6-4a34-9105-daa0ebd62a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'강아지': 1, '고양이': 2, '애교': 3, '컴퓨터': 4, '노트북': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(t.word_index) + 1\n",
        "print(\"슝=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVyCuTspyXUY",
        "outputId": "674b8bf0-9e45-4b56-d663-ed54f9dbc871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- vocab_size +1 이유\n",
        " - 자연어 처리 시 0번 단어는 특별 토근으로 단어장 추가가 대부분임.\n",
        " - 0번은 padding 작업을 위한 패딩 토큰으로 사용됨."
      ],
      "metadata": {
        "id": "4hmbD2JabRlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_text = ['강아지', '고양이', '강아지', '컴퓨터']\n",
        "encoded = t.texts_to_sequences([sub_text])\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaKmcGEMybuG",
        "outputId": "e05bd9ec-56d6-4fb4-c76b-d46638cbb37e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 1, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot = to_categorical(encoded, num_classes = vocab_size)\n",
        "print(one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKaAztTCyc10",
        "outputId": "8c4d9de5-a640-4c53-be1d-740131390196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 워드 임베딩\n",
        "\n",
        "📌 원-핫 벡터의 한계\n",
        "\n",
        "단어를 고유한 정수 인덱스로 표현 → 대부분 0이고 한 개만 1인 희소 벡터(sparse vector).\n",
        "\n",
        "단어 간 의미적 유사성을 표현할 수 없음.\n",
        "\n",
        "예: ‘강아지’와 ‘고양이’는 유사하지만, 원-핫 벡터에서는 내적이 0으로 모든 단어가 직교(orthogonal) → 유사도 표현 불가.\n",
        "\n",
        "따라서 기계는 의미를 이해하거나 새로운 문장을 생성하는 데 어려움이 있음.\n",
        "\n",
        "\n",
        "📌 워드 임베딩(Word Embedding)의 등장\n",
        "\n",
        "단어를 고정된 길이의 밀집 벡터(dense vector) 로 표현.\n",
        "\n",
        "대부분의 값이 0이 아닌 실수값 → 희소 벡터의 반대 개념.\n",
        "\n",
        "단어 간 의미적 유사성이나 관계를 벡터 공간에서 나타낼 수 있음.\n",
        "\n",
        "벡터 간 내적, 거리 등을 통해 유사성을 계산할 수 있음.\n",
        "\n",
        "📌 워드 임베딩의 특징\n",
        "\n",
        "벡터의 각 요소는 사람 눈에는 의미 해석이 어려우나, 다양한 의미를 내포.\n",
        "\n",
        "예: 사과 = [0.8, 0.7, 0.7, 0.1] → \"둥글고, 빨갛고, 달고, 조금 신\" 특성을 내포 (예시일 뿐 실제 벡터는 다름).\n",
        "\n",
        "기계는 학습을 통해 단어 벡터를 반복적으로 조정하여 의미를 잘 담도록 함.\n",
        "\n",
        "신경망이나 통계 기반 기법 사용.\n",
        "\n",
        "마치 딥러닝의 가중치를 학습하는 것처럼.\n",
        "\n",
        "✅ 결론\n",
        "\n",
        "워드 임베딩은\n",
        "\n",
        "1) 단어를 짧은 밀집 벡터로 표현하고,\n",
        "\n",
        "2) 그 벡터에 의미나 관계를 내포시키는 것이 핵심."
      ],
      "metadata": {
        "id": "1tdFJvB3bo9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Word2Vec\n",
        " - 텍스트 데이터 다루기 - Word2Vec 단어로 벡터 만들기 : https://record3329.tistory.com/29"
      ],
      "metadata": {
        "id": "zPTljmVifeew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 분포가설\n",
        " -  ‘비슷한 문맥에서 같이 등장하는 경향이 있는 단어들은 비슷한 의미를 가진다.’"
      ],
      "metadata": {
        "id": "NR9dPS6jfg4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CBoW(Continuous Bag of words)\n",
        "- 정의 : 주변에 있는 단어들을 통해 중간에 있는 단어들을 예측하는 방법\n",
        "\n"
      ],
      "metadata": {
        "id": "lVWAzGmGi96t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skip-Gram\n",
        "- 중간에 있는 단어로 주변 단어들을 예측하는 방법\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QvXvBysajJsw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### negative sampling\n",
        " - 주어진 데이터에서 무작위로 음성(negative) 샘플을 추출하는 방법\n",
        " - 양성 샘플과 음성 샘플의 수를 균형있게 맞춰서 모델이 학습할 수 있는 환경을 만듬.\n",
        " - 단순히 무작위로 샘플링을 하는 것이 아니라, 더 중요한 샘플에 더 많은 가중치를 부여하여 샘플링 비율을 조절할 수 있음.\n",
        " - 모델이 보다 중요한 샘플에 더 집중하도록 함"
      ],
      "metadata": {
        "id": "jdxETFunyZTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('abc')\n",
        "\n",
        "## 이부분 punkt 대신 punkt_tab을 이용\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egjUWoRmyd8I",
        "outputId": "dab4a23e-25be-4b1a-e87f-da341d10fb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/abc.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import abc\n",
        "corpus = abc.sents()\n",
        "print(\"슝~\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH055v5Yy1J3",
        "outputId": "d297fb4d-27e8-4164-f093-e4b57d0cddee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hb6cSjpzb8p",
        "outputId": "24355a14-fde0-4aa8-de4e-bcc720c7e068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['PM', 'denies', 'knowledge', 'of', 'AWB', 'kickbacks', 'The', 'Prime', 'Minister', 'has', 'denied', 'he', 'knew', 'AWB', 'was', 'paying', 'kickbacks', 'to', 'Iraq', 'despite', 'writing', 'to', 'the', 'wheat', 'exporter', 'asking', 'to', 'be', 'kept', 'fully', 'informed', 'on', 'Iraq', 'wheat', 'sales', '.'], ['Letters', 'from', 'John', 'Howard', 'and', 'Deputy', 'Prime', 'Minister', 'Mark', 'Vaile', 'to', 'AWB', 'have', 'been', 'released', 'by', 'the', 'Cole', 'inquiry', 'into', 'the', 'oil', 'for', 'food', 'program', '.'], ['In', 'one', 'of', 'the', 'letters', 'Mr', 'Howard', 'asks', 'AWB', 'managing', 'director', 'Andrew', 'Lindberg', 'to', 'remain', 'in', 'close', 'contact', 'with', 'the', 'Government', 'on', 'Iraq', 'wheat', 'sales', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('코퍼스의 크기 :',len(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKFTFco3zdBq",
        "outputId": "6e2b7a65-14bf-461b-990d-b17ff841404e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "코퍼스의 크기 : 29059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(sentences = corpus, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)\n",
        "print(\"모델 학습 완료!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8rns_DfzeB4",
        "outputId": "0758c768-470a-464c-f289-7d7e6e265bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 학습 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 파라메터의 의미\n",
        "  - vector size = 학습 후 임베딩 벡터의 차원\n",
        "  - window = 컨텍스트 윈도우 크기\n",
        "  - min_count = 단어 최소 빈도수 제한 (빈도가 적은 단어들은 학습하지 않음.)\n",
        "  - workers = 학습을 위한 프로세스 수\n",
        "  - sg = 0은 CBoW, 1은 Skip-gram."
      ],
      "metadata": {
        "id": "gMXlLAkBqtro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_result = model.wv.most_similar(\"man\")\n",
        "print(model_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hK4SvvQz6mS",
        "outputId": "00ef5cf6-e328-4e41-f5e9-f200b493e192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.923514723777771), ('skull', 0.9112710952758789), ('Bang', 0.9060195684432983), ('asteroid', 0.9052225947380066), ('third', 0.9020858407020569), ('baby', 0.8995046615600586), ('dog', 0.8983686566352844), ('bought', 0.8971353769302368), ('rally', 0.8909624218940735), ('dinosaur', 0.8893425464630127)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "### 코랩이라 경로 바꿔줘야합니다\n",
        "model.wv.save_word2vec_format('./w2v')\n",
        "loaded_model = KeyedVectors.load_word2vec_format(\"./w2v\")\n",
        "print(\"모델  load 완료!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2zvf2OPz9Bj",
        "outputId": "3e419d6d-0cd6-4d5b-83ce-7a57aac9c1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델  load 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_result = loaded_model.most_similar(\"man\")\n",
        "print(model_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB2SmbNz0E-n",
        "outputId": "7b32b027-dd99-452b-b7c5-66d64b29015e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.923514723777771), ('skull', 0.9112710952758789), ('Bang', 0.9060195684432983), ('asteroid', 0.9052225947380066), ('third', 0.9020858407020569), ('baby', 0.8995046615600586), ('dog', 0.8983686566352844), ('bought', 0.8971353769302368), ('rally', 0.8909624218940735), ('dinosaur', 0.8893425464630127)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- out of vacabuary 문제"
      ],
      "metadata": {
        "id": "RYtJc14Mq-hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 에러나는 코드들\n",
        "# 에러가 나더라도 놀라지 마세요.\n",
        "loaded_model.most_similar('overacting')\n",
        "loaded_model.most_similar('memorry')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "8XmMQx4Q0Ggd",
        "outputId": "a0f8d909-83b5-4c74-bf4b-0fadb28a6e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Key 'overacting' not present in vocabulary\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-628741850>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 에러나는 코드들\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 에러가 나더라도 놀라지 마세요.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overacting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'memorry'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'overacting' not present in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 임베딩 벡터의 시각화"
      ],
      "metadata": {
        "id": "aEz-GzENrKPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word2vec 모델 메타정보 및 텐서 내보내기\n",
        "!python -m gensim.scripts.word2vec2tensor --input ./w2v --output ./w2v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4hCGUCg0MFG",
        "outputId": "be415f30-3789-4ba7-b856-3059ecd2a2d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-17 06:57:21,061 - word2vec2tensor - INFO - running /usr/local/lib/python3.11/dist-packages/gensim/scripts/word2vec2tensor.py --input ./w2v --output ./w2v\n",
            "2025-06-17 06:57:21,061 - keyedvectors - INFO - loading projection weights from ./w2v\n",
            "2025-06-17 06:57:21,694 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (10363, 100) matrix of type float32 from ./w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-06-17T06:57:21.686795', 'gensim': '4.3.3', 'python': '3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]', 'platform': 'Linux-6.1.123+-x86_64-with-glibc2.35', 'event': 'load_word2vec_format'}\n",
            "2025-06-17 06:57:22,423 - word2vec2tensor - INFO - 2D tensor file saved to ./w2v_tensor.tsv\n",
            "2025-06-17 06:57:22,423 - word2vec2tensor - INFO - Tensor metadata file saved to ./w2v_metadata.tsv\n",
            "2025-06-17 06:57:22,424 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FastText\n",
        "- Wore2Vec 이후 등장한 워드 임베딩 방법\n",
        "- 문자 단위 n-gram(character-level n-gram) 표현을 학습함\n",
        "- Word2Vec은 더이상 깨질 수 없는 단위로 구분\n",
        "  \n",
        "  vs FastText 는 내부 단어(subwords)들을 학습한다\n",
        "\n",
        "- 오타에 강건(robust)함\n",
        "\n",
        "- OOV(Out-Of-Vocabulary) 문제 : 기계학습 모델이 훈련 데이터에서는 보지 못했지만, 테스트 데이터에 등장하는 단어를 인식하지 못하는 문제\n",
        "\n",
        "- **FastText는 OOV 문제를 해결할 수 있음**.\n",
        "- 왜냐하면 훈련 데이터에서 등장하지 않은 단어라도 서브워드로 분해하면서 해당 서브워드를 포함하고 있는 다른 단어들의 임베딩 벡터를 활용하여 새로운 단어의 임베딩 벡터를 만들기 때문임."
      ],
      "metadata": {
        "id": "64uwRJz5st83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "fasttext_model = FastText(corpus, window=5, min_count=5, workers=4, sg=1)\n",
        "print(\"FastText 학습 완료!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCwipTr30zC_",
        "outputId": "1fcf0d3c-82cf-4283-f4ec-ed33d4249b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText 학습 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext_model.wv.most_similar('overacting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3Cx0B2w1E-B",
        "outputId": "d0a7bcf7-6407-4432-f768-fc8f84e23cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('extracting', 0.9424561262130737),\n",
              " ('shooting', 0.9382485151290894),\n",
              " ('lifting', 0.9375076293945312),\n",
              " ('fluctuating', 0.9359311461448669),\n",
              " ('mixing', 0.9353761672973633),\n",
              " ('losing', 0.9350128769874573),\n",
              " ('attracting', 0.9310552477836609),\n",
              " ('shifting', 0.9307162165641785),\n",
              " ('negotiating', 0.9292001724243164),\n",
              " ('emptying', 0.9288997054100037)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext_model.wv.most_similar('memoryy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Su2Ceij1GT7",
        "outputId": "eb554e1b-32be-4c04-aa74-9eea231860ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('memory', 0.9544547200202942),\n",
              " ('musical', 0.862307608127594),\n",
              " ('intelligence', 0.8490725755691528),\n",
              " ('sense', 0.8451464772224426),\n",
              " ('mechanical', 0.8398575186729431),\n",
              " ('mechanism', 0.8366336226463318),\n",
              " ('basic', 0.8358100652694702),\n",
              " ('mechanisms', 0.8347879648208618),\n",
              " ('sensitivity', 0.831736147403717),\n",
              " ('music', 0.8314796090126038)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe(글로브, Global Vectors for Word Representation)\n",
        " - 2014년에 미국 스탠포드 대학에서 개발한 워드 임베딩 방법론\n",
        " - 특징 : 카운트 기반과 예측 기반 두 가지 방법을 모두 사용"
      ],
      "metadata": {
        "id": "C1pP6MFGuoh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "glove_model = api.load(\"glove-wiki-gigaword-50\")  # glove vectors 다운로드\n",
        "glove_model.most_similar(\"dog\")  # 'dog'과 비슷한 단어 찾기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AB2hz_s1Hrl",
        "outputId": "ea2c2ee2-0942-4374-b7f9-15c2c2a33888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cat', 0.9218004941940308),\n",
              " ('dogs', 0.8513158559799194),\n",
              " ('horse', 0.7907583713531494),\n",
              " ('puppy', 0.7754920721054077),\n",
              " ('pet', 0.7724708318710327),\n",
              " ('rabbit', 0.7720814347267151),\n",
              " ('pig', 0.7490062117576599),\n",
              " ('snake', 0.7399188876152039),\n",
              " ('baby', 0.7395570278167725),\n",
              " ('bite', 0.7387937307357788)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_model.most_similar('overacting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beg3s-wD1Kum",
        "outputId": "aff3894f-77e3-48d6-83c2-eaeb2f3beb40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('impudence', 0.7842012047767639),\n",
              " ('puerile', 0.7816032767295837),\n",
              " ('winningly', 0.7644237875938416),\n",
              " ('grossness', 0.7576098442077637),\n",
              " ('deconstructions', 0.748936653137207),\n",
              " ('over-the-top', 0.7460805773735046),\n",
              " ('buffoonery', 0.746045708656311),\n",
              " ('impetuosity', 0.7415392398834229),\n",
              " ('sophomoric', 0.736961841583252),\n",
              " ('zaniness', 0.7353197336196899)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_model.most_similar('memoryy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "noIb3XIR1Lmf",
        "outputId": "22bbfd91-f1e7-4953-be2e-351520534731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Key 'memoryy' not present in vocabulary\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-4082020240>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mglove_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'memoryy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'memoryy' not present in vocabulary\""
          ]
        }
      ]
    }
  ]
}